# Transformer-Based Detection and Defense Against Prompt Injection Attacks

A research project investigating the use of transformer models to detect and defend against prompt injection attacks on Large Language Models.

## ğŸ¯ Project Overview

This project implements and evaluates multiple transformer architectures for detecting prompt injection attacks in real-time. We also explore defensive mechanisms including prompt sanitization and adversarial robustness techniques.

**Research Questions:**
- Can transformers effectively detect prompt injections with high accuracy and low false positives?
- How do different architectures compare for this security task?
- What linguistic features do models learn to identify attacks?
- Can we sanitize malicious prompts while preserving legitimate user intent?

## ğŸ“‹ Requirements

- Python 3.11+
- CUDA-capable GPU (recommended: 16GB+ VRAM)
- 20GB disk space for datasets and models

## ğŸš€ Quick Start - Reproduce All Experiments

**One-line reproduction:**
```bash
python main.py --run-all --output-dir results/
```

This will:
1. Download and prepare datasets
2. Train all models (BERT, RoBERTa, DistilBERT, DeBERTa)
3. Run all ablation studies
4. Generate evaluation metrics and visualizations
5. Save results to `results/` directory

**Estimated runtime:** ~40 hours on RTX 3090 (or ~20 hours on A100)

## ğŸ“¦ Installation

### Step 1: Clone the repository
```bash
git clone https://github.com/yourusername/prompt-injection-detection.git
cd prompt-injection-detection
```

### Step 2: Create virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download datasets (optional - auto-downloads if not present)
```bash
python scripts/download_data.py
```

## ğŸ”¬ Running Individual Experiments

### Experiment 1: Architecture Comparison
```bash
python main.py --experiment detection_comparison --models bert roberta distilbert deberta
```

### Experiment 2: Attack Type Analysis
```bash
python main.py --experiment attack_analysis --model roberta
```

### Experiment 3: Interpretability Analysis
```bash
python main.py --experiment interpretability --model roberta --visualize
```

### Experiment 4: Sanitization Model
```bash
python main.py --experiment sanitization --model t5-small
```

### Experiment 5: Ablation Studies
```bash
# Ablation: Pre-training importance
python main.py --ablation pretrain --model bert

# Ablation: Context window size
python main.py --ablation context --sizes 64 128 256 512

# Ablation: Training data diversity
python main.py --ablation data_diversity

# Ablation: Attention mechanism
python main.py --ablation attention --compare lstm local full
```

### Experiment 6: Adversarial Robustness
```bash
python main.py --experiment adversarial --model roberta --attacks paraphrase substitute gradient
```

## ğŸ“Š Project Structure

```
prompt-injection-detection/
â”œâ”€â”€ main.py                          # Main entry point for all experiments
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ EXPERIMENT_DESIGN.md            # Detailed experimental design
â”‚
â”œâ”€â”€ data/                            # Datasets (auto-downloaded)
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ val.jsonl
â”‚   â”œâ”€â”€ test.jsonl
â”‚   â””â”€â”€ adversarial_test.jsonl
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py              # Dataset loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ generators.py           # Synthetic attack generation
â”‚   â”‚   â””â”€â”€ augmentation.py         # Data augmentation
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ classifier.py           # Transformer classifiers
â”‚   â”‚   â”œâ”€â”€ sanitizer.py            # Seq2seq sanitization model
â”‚   â”‚   â””â”€â”€ custom_transformer.py   # Small custom transformer
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py              # Training loop
â”‚   â”‚   â”œâ”€â”€ evaluator.py            # Evaluation metrics
â”‚   â”‚   â””â”€â”€ ablations.py            # Ablation study implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ interpretability/
â”‚   â”‚   â”œâ”€â”€ attention_viz.py        # Attention visualization
â”‚   â”‚   â”œâ”€â”€ saliency.py             # Gradient-based saliency
â”‚   â”‚   â””â”€â”€ probing.py              # Probing classifiers
â”‚   â”‚
â”‚   â”œâ”€â”€ attacks/
â”‚   â”‚   â”œâ”€â”€ adversarial.py          # Adversarial attack generation
â”‚   â”‚   â””â”€â”€ evasion_tests.py        # Evasion testing
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py              # Custom metrics
â”‚       â”œâ”€â”€ visualization.py        # Plotting utilities
â”‚       â””â”€â”€ config.py               # Configuration management
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_data.py            # Dataset download script
â”‚   â”œâ”€â”€ generate_attacks.py         # Attack generation
â”‚   â””â”€â”€ evaluate_all.py             # Batch evaluation
â”‚
â”œâ”€â”€ tests/                          # Unit tests
â”‚   â”œâ”€â”€ test_models.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_attacks.py
â”‚
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for analysis
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ results_visualization.ipynb
â”‚   â””â”€â”€ case_studies.ipynb
â”‚
â”œâ”€â”€ results/                        # Experiment outputs (gitignored)
â”‚   â”œâ”€â”€ models/                     # Trained model checkpoints
â”‚   â”œâ”€â”€ metrics/                    # Evaluation results
â”‚   â”œâ”€â”€ visualizations/             # Plots and figures
â”‚   â””â”€â”€ logs/                       # Training logs
â”‚
â””â”€â”€ paper/                          # LaTeX paper
    â”œâ”€â”€ main.tex
    â”œâ”€â”€ sections/
    â”œâ”€â”€ figures/
    â””â”€â”€ references.bib
```

## ğŸ”§ Configuration

Edit `config.yaml` to customize:
- Model architectures and hyperparameters
- Training settings (batch size, learning rate, epochs)
- Dataset paths and split ratios
- Evaluation metrics and thresholds

Example:
```yaml
training:
  batch_size: 32
  learning_rate: 2e-5
  epochs: 5
  early_stopping_patience: 2

models:
  bert:
    pretrained: "bert-base-uncased"
    max_length: 512
  roberta:
    pretrained: "roberta-base"
    max_length: 512
```

## ğŸ“ˆ Results

After running experiments, view results:

```bash
# Generate summary report
python scripts/generate_report.py --results-dir results/

# Launch visualization dashboard
python scripts/dashboard.py --port 8080
```

Results will include:
- Model comparison table (accuracy, precision, recall, F1, latency)
- Per-attack-type performance breakdown
- Attention visualizations
- Ablation study results
- Adversarial robustness scores

## ğŸ§ª Testing

Run unit tests:
```bash
pytest tests/
```

Run integration tests:
```bash
pytest tests/ --integration
```

## ğŸ“ Citation

If you use this code or dataset in your research, please cite:

```bibtex
@misc{prompt-injection-detection-2025,
  author = {Your Name},
  title = {Transformer-Based Detection and Defense Against Prompt Injection Attacks},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/yourusername/prompt-injection-detection}
}
```

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

This is a research project. For questions or collaboration:
- Open an issue on GitHub
- Contact: your.email@university.edu

## ğŸ™ Acknowledgments

- Datasets: ShareGPT, Alpaca, LMSYS
- Pre-trained models: HuggingFace Transformers
- Inspiration: Recent work on LLM security (see EXPERIMENT_DESIGN.md for references)

## ğŸ“š Additional Resources

- **Experiment Design:** See `EXPERIMENT_DESIGN.md` for detailed methodology
- **Paper Draft:** See `paper/main.tex` for LaTeX source
- **Notebooks:** See `notebooks/` for exploratory analysis and visualization

---

**Status:** ğŸš§ Under Development

**Last Updated:** December 2025

