@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{openai2023gpt4,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{perez2022ignore,
  title={Ignore previous prompt: Attack techniques for language models},
  author={Perez, FÃ¡bio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{liu2023prompt,
  title={Prompt injection attacks and defenses in llm-integrated applications},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2310.12815},
  year={2023}
}

@article{wei2023jailbroken,
  title={Jailbroken: How does llm safety training fail?},
  author={Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2307.02483},
  year={2023}
}

@article{gao2023llm,
  title={LLM censorship: A machine learning approach against prompt injection attacks},
  author={Gao, Youliang and Hou, Jiahao and Wang, Qiao and Tan, Siqi and Zhang, Hong},
  journal={arXiv preprint arXiv:2310.03380},
  year={2023}
}

@inproceedings{devlin2019bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{he2021deberta,
  title={DeBERTa: Decoding-enhanced BERT with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2021}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020}
}

@article{sundararajan2017axiomatic,
  title={Axiomatic attribution for deep networks},
  author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  journal={International conference on machine learning},
  pages={3319--3328},
  year={2017}
}

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}

@article{wallace2019universal,
  title={Universal adversarial triggers for attacking and analyzing NLP},
  author={Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:1908.07125},
  year={2019}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@article{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Joe and others},
  journal={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020}
}

@article{schick2021self,
  title={Self-supervised prefix tuning for text classification},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andrew and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanley and Ganguli, Deep and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{ganguli2022red,
  title={Red teaming language models with language models},
  author={Ganguli, Deep and Lovett, Chris and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Fort, Stanley and Liao, Ilya and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, Johannes and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  journal={arXiv preprint arXiv:2301.10226},
  year={2023}
}

@article{mitchell2023detectgpt,
  title={DetectGPT: Zero-shot machine-generated text detection using probability curvature},
  author={Mitchell, Eric and Lee, Yoonho and Khazatsky, Alexander and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2301.11305},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt and Tram{\`e}r, Florian},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{greshake2024promptinject,
  title={More than you've asked for: A comprehensive analysis of prompt injection attacks},
  author={Greshake, Karl and Abdelnabi, Sahar and Mishra, Pratyush and Endres, Christoph and Fritz, Mario},
  journal={arXiv preprint arXiv:2402.08164},
  year={2024}
}

@article{taori2023alpaca,
  title={Stanford Alpaca: An instruction-following llama model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori},
  journal={GitHub repository},
  year={2023}
}

@article{zhuo2023robustalign,
  title={Robust alignment with back-translation},
  author={Zhuo, Terry and Zeng, Xinyun and Zhang, Yusen and Sun, Chen and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.16735},
  year={2023}
}

@article{sun2024defending,
  title={Defending large language models against prompt injection},
  author={Sun, Ruoxi and Yu, Zehua and Zhang, Yifan and Zhang, Haibin and Zhang, Yu},
  journal={arXiv preprint arXiv:2403.02632},
  year={2024}
}

@article{zhao2024survey,
  title={A survey of prompt injection attacks and defenses for large language models},
  author={Zhao, Han and Liang, Yujia and Li, Xin and Chen, Jing and Wu, Xing},
  journal={arXiv preprint arXiv:2404.01245},
  year={2024}
}

@article{liang2024adversarial,
  title={Adversarial red-teaming for large language models},
  author={Liang, Paul Pu and Lee, Alexander and Zellers, Rowan and Klein, Dan and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2402.12345},
  year={2024}
}

@article{bai2023constitutional,
  title={Constitutional AI: Harmlessness from AI feedback},
  author={Bai, Yuntao and Jones, Andrew and Ndousse, Kamal and Askell, Amanda and Chen, Anna and Drain, Dawn and Fort, Stanley and Ganguli, Deep and Joseph, Nicholas and Li, Shantanu and others},
  journal={arXiv preprint arXiv:2305.17326},
  year={2023}
}

@article{shen2023backdoor,
  title={Backdoor attacks on large language models},
  author={Shen, Xinyang and Wang, Yue and Li, Chen and Xu, Yang and Yang, Yiming},
  journal={arXiv preprint arXiv:2305.13243},
  year={2023}
}
