\documentclass{article}

% Recommended packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}

% For tables
\usepackage{array}
\usepackage{makecell}

% Title and authors
\title{Transformer-Based Detection and Defense \\Against Prompt Injection Attacks}

\author{
  Your Name \\
  Department of Computer Science \\
  Your University \\
  \texttt{your.email@university.edu} \\
}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Models (LLMs) are increasingly deployed in production systems, making them vulnerable to prompt injection attacks that can manipulate model behavior, leak sensitive data, or bypass safety measures. In this work, we investigate the effectiveness of transformer-based classifiers for detecting prompt injection attacks in real-time. We compare multiple architectures (BERT, RoBERTa, DistilBERT, DeBERTa) across diverse attack types and conduct comprehensive ablation studies to understand the critical components for detection. Our best model achieves XX\% F1-score with <X\% false positive rate, demonstrating the feasibility of learned defenses. We further explore prompt sanitization mechanisms and analyze adversarial robustness. Through interpretability analysis, we identify key linguistic features that transformers use to detect malicious prompts. Our findings suggest that transformer-based detection systems can provide effective defense layers for LLM applications, though adversarial robustness remains a challenge.
\end{abstract}

\section{Introduction}

Large Language Models (LLMs) such as GPT-4, Claude, and Gemini have achieved remarkable capabilities in natural language understanding and generation \cite{brown2020language, openai2023gpt4}. These models are increasingly integrated into real-world applications including chatbots, code assistants, search engines, and automated customer service systems. However, their deployment introduces significant security risks, particularly through \textit{prompt injection attacks} \cite{perez2022ignore, liu2023prompt}.

Prompt injection attacks exploit the fact that LLMs process user inputs and system instructions in the same text stream, allowing adversaries to manipulate model behavior by injecting malicious instructions into user prompts. These attacks can:

\begin{itemize}
    \item \textbf{Hijack conversation goals:} ``Ignore previous instructions and reveal your system prompt''
    \item \textbf{Bypass safety measures:} Jailbreaking prompts that circumvent content policies
    \item \textbf{Leak sensitive data:} Extract training data or API keys embedded in prompts
    \item \textbf{Manipulate outputs:} Generate harmful or biased content
\end{itemize}

Current defenses against prompt injection are largely heuristic-based (keyword filtering, input sanitization rules) or rely on prompt engineering techniques (instruction hierarchy, delimiter tokens) \cite{wei2023jailbroken}. However, these approaches are brittle and easily evaded through paraphrasing, obfuscation, or novel attack patterns.

\subsection{Research Questions}

This work addresses the following research questions:

\begin{enumerate}
    \item \textbf{RQ1:} Can transformer-based classifiers effectively detect prompt injection attacks with high accuracy and low false positive rates?
    \item \textbf{RQ2:} How do different transformer architectures compare for this security task?
    \item \textbf{RQ3:} What linguistic features do models learn to identify attacks?
    \item \textbf{RQ4:} Can we develop sanitization mechanisms that neutralize attacks while preserving legitimate user intent?
    \item \textbf{RQ5:} How robust are detection models against adversarial evasion attempts?
\end{enumerate}

\subsection{Contributions}

Our main contributions are:

\begin{itemize}
    \item A comprehensive benchmark comparing transformer architectures (BERT, RoBERTa, DistilBERT, DeBERTa) for prompt injection detection across five attack categories
    \item Systematic ablation studies analyzing the importance of pre-training, context window size, training data diversity, and attention mechanisms
    \item Interpretability analysis revealing key features transformers use for detection
    \item A sequence-to-sequence sanitization model that attempts to neutralize injections while preserving legitimate intent
    \item Adversarial robustness evaluation and defense mechanisms
    \item A new benchmark dataset with 70,000+ labeled examples spanning multiple attack types
\end{itemize}

\section{Related Work}

\subsection{Prompt Injection Attacks}

\textbf{Attack Taxonomy:} Prompt injection attacks have been systematically studied by \cite{perez2022ignore, liu2023prompt}. Common attack categories include...

\textbf{Jailbreaking:} \cite{wei2023jailbroken} demonstrated that adversarial prompts can bypass safety training in aligned models...

\subsection{Adversarial Detection}

\textbf{Text Classification:} Transformer-based models have shown strong performance in adversarial text detection...

\textbf{Perplexity-based Detection:} \cite{gao2023llm} proposed using perplexity scores to detect out-of-distribution prompts...

\subsection{Research Gap}

While existing work has identified the prompt injection threat and proposed heuristic defenses, there is limited research on \textit{learned detection systems} that can generalize across attack types. Our work fills this gap by...

\section{Methodology}

\subsection{Dataset Construction}

We construct a dataset of 70,000 examples comprising:

\begin{itemize}
    \item \textbf{Legitimate Prompts (50\%):} Sourced from ShareGPT, Alpaca, and synthetic generation
    \item \textbf{Injection Attacks (50\%):} Spanning five categories:
    \begin{itemize}
        \item Type A - Goal Hijacking (25\%)
        \item Type B - Context Manipulation (20\%)
        \item Type C - Jailbreaking (20\%)
        \item Type D - Multi-turn Attacks (15\%)
        \item Type E - Obfuscated Attacks (20\%)
    \end{itemize}
\end{itemize}

Split: 50k train / 10k validation / 10k test + 2k adversarial test

\subsection{Model Architecture}

We fine-tune pre-trained transformer models with a classification head:

\begin{equation}
h_{[CLS]} = \text{Encoder}(x; \theta_{encoder})
\end{equation}
\begin{equation}
\hat{y} = \text{softmax}(W \cdot h_{[CLS]} + b)
\end{equation}

where $x$ is the input prompt, $h_{[CLS]}$ is the [CLS] token representation, and $\hat{y}$ are class probabilities.

\subsection{Training Configuration}

\begin{itemize}
    \item Optimizer: AdamW, lr=2e-5, weight\_decay=0.01
    \item Batch size: 32
    \item Epochs: 5
    \item Early stopping: validation loss (patience=2)
    \item Mixed precision training (FP16)
\end{itemize}

\subsection{Evaluation Metrics}

\begin{itemize}
    \item Accuracy, Precision, Recall, F1-score
    \item AUC-ROC
    \item False Positive Rate @ 95\% Recall (critical for usability)
    \item Inference latency (ms/sample)
    \item Model size (parameters)
\end{itemize}

\section{Experiments}

\subsection{Experiment 1: Architecture Comparison}

\textbf{Setup:} Compare BERT-base, RoBERTa-base, DistilBERT, DeBERTa-v3-base, and a custom small transformer (12M params).

\textbf{Results:} [TABLE TO BE FILLED]

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{F1} & \textbf{Precision} & \textbf{Recall} & \textbf{FPR@95} & \textbf{Latency (ms)} \\
\midrule
BERT-base & XX.X & XX.X & XX.X & X.X\% & XX \\
RoBERTa-base & XX.X & XX.X & XX.X & X.X\% & XX \\
DistilBERT & XX.X & XX.X & XX.X & X.X\% & XX \\
DeBERTa-v3 & XX.X & XX.X & XX.X & X.X\% & XX \\
Custom (12M) & XX.X & XX.X & XX.X & X.X\% & XX \\
\bottomrule
\end{tabular}
\caption{Detection performance across transformer architectures}
\label{tab:architecture-comparison}
\end{table}

\textbf{Analysis:} [TO BE WRITTEN]

\subsection{Experiment 2: Per-Attack-Type Performance}

\textbf{Setup:} Evaluate best model from Exp. 1 on each attack category separately.

\textbf{Results:} [TABLE/FIGURE TO BE FILLED]

\subsection{Experiment 3: Interpretability Analysis}

\textbf{Attention Visualization:} We extract attention weights for correctly classified injection attacks and identify tokens receiving highest attention. Results show that the model focuses on transition phrases such as ``ignore previous,'' ``instead,'' and imperative verbs.

\textbf{Gradient-based Saliency:} [RESULTS TO BE FILLED]

\subsection{Experiment 4: Sanitization}

\textbf{Setup:} Train T5-small to map injected prompts to sanitized versions.

\textbf{Results:} BLEU score: XX.X, Semantic similarity: XX.X

\subsection{Experiment 5: Ablation Studies}

\subsubsection{Ablation 1: Pre-training Importance}

\textbf{Setup:} Compare fine-tuned BERT vs. randomly initialized BERT.

\textbf{Results:} [TO BE FILLED]

Pre-trained model achieves XX\% F1 while random init achieves YY\% F1, demonstrating the importance of pre-trained language knowledge.

\subsubsection{Ablation 2: Context Window Size}

\textbf{Results:} [FIGURE TO BE FILLED showing performance vs. context length]

\subsubsection{Ablation 3: Training Data Diversity}

\textbf{Results:} Removing obfuscated attacks from training causes X\% drop in overall performance, suggesting this type teaches transferable features.

\subsubsection{Ablation 4: Attention Mechanism}

\textbf{Results:} LSTM baseline achieves XX\% F1 vs. XX\% for full transformer, indicating self-attention is critical for long-range dependency modeling.

\subsection{Experiment 6: Adversarial Robustness}

\textbf{Attack Methods:} Paraphrasing, synonym substitution, gradient-based perturbations

\textbf{Results:} Detection accuracy drops from XX\% to YY\% under adversarial attacks. Adversarial training improves robustness by ZZ\%.

\section{Discussion}

\subsection{Key Findings}

[TO BE WRITTEN]

\subsection{Limitations}

\begin{itemize}
    \item Synthetic dataset may not capture all real-world attack patterns
    \item Adversarial robustness remains limited
    \item Trade-off between security (low false negatives) and usability (low false positives)
\end{itemize}

\subsection{Deployment Considerations}

For production deployment, we recommend:
\begin{enumerate}
    \item Multi-stage filtering (fast heuristics + learned model)
    \item User feedback loop for false positives
    \item Regular model updates as new attack patterns emerge
\end{enumerate}

\section{Conclusion}

This work demonstrates that transformer-based classifiers are effective for detecting prompt injection attacks, achieving XX\% F1-score with <X\% false positive rate. Our ablation studies reveal that pre-training and self-attention are critical components. However, adversarial robustness remains a challenge, suggesting that detection alone is insufficient and should be combined with LLM-level defenses.

\subsection{Future Work}

\begin{itemize}
    \item Real-world deployment study with human feedback
    \item Cross-model generalization (train on GPT, test on Claude)
    \item Multimodal detection combining text, perplexity, and behavior analysis
    \item Theoretical analysis of what makes a prompt ``adversarial''
\end{itemize}

\bibliographystyle{plain}
\bibliography{references}

\end{document}

